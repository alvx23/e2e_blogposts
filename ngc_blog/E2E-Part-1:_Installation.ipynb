{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef88a6b-8356-4e68-a929-eb929655b021",
   "metadata": {},
   "source": [
    "# End-to-End Example training object detection model using NVIDIA Pytorch Container from NGC\n",
    " ----\n",
    "\n",
    "Note this Object Detection demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Install the Docker Engine on your system\n",
    "* Pull a Pytorch container from the NGC Catalog using Docker\n",
    "* Run the Pytorch container using Docker\n",
    "* Part 2 : Preprocess the Satellite imagery object detection dataset using the SAHI library\n",
    "* Part 3 : Execute training a object detection on satellite imagery using TensorFlow and Jupyter Notebook\n",
    "* Part 4 : Run inference on a trained object detection model using the SAHI library\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Install the Docker Engine\n",
    "Go to https://docs.docker.com/engine/install/ to install the Docker Engine on your system.\n",
    "\n",
    "\n",
    "### 2. Download the TensorFlow container from the NGC Catalog \n",
    "\n",
    "Once the Docker Engine is installed on your machine, visit https://ngc.nvidia.com/catalog/containers and search for the TensorFlow container. Click on the TensorFlow card and copy the pull command.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/NGC.png\">\n",
    "\n",
    "Open the command line of your machine and past the pull command into your command line. Execute the command to download the container. \n",
    "\n",
    "```$ docker pull nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "    \n",
    "The container starts downloading to your computer. A container image consists of many layers; all of them need to be pulled. \n",
    "\n",
    "### 3. Run the TensorFlow container image\n",
    "\n",
    "Once the container download is completed, run the following code in your command line to run and start the container:\n",
    "\n",
    "```$ docker run -it --gpus all  -p 8888:8888 -v $PWD:/projects --network=host nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline1.png\">\n",
    "\n",
    "### 4. Install Jupyter lab and open a notebook\n",
    "\n",
    "Within the container, run the following commands:\n",
    "\n",
    "```pip install torchvision==0.11.3 jupyterlab```\n",
    "\n",
    "```jupyter lab --ip=0.0.0.0 --port=8888 --allow-root```\n",
    "\n",
    "Open up your favorite browser and enter:Â http://localhost:8888/?token=*yourtoken*.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline2.png\">\n",
    "\n",
    "You should see the Jupyter Lab application. Click on the plus icon to launch a new Python 3 noteb\n",
    "ook.\n",
    "\n",
    "Follow along with the image classification with the TensorFlow example provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f70686f-3a43-4ede-8526-03684ec6128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cython pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46ad70-6d9d-41bf-8b69-735c1f34b361",
   "metadata": {},
   "source": [
    "# TLDR; run training job on 8 GPUS\n",
    "The below cell will run a multi-gpu training job. This job will train an object detection model (faster-rcnn) on a dataset of satellite imagery images that contain 61 classes of objects\n",
    "* Change `nproc_per_node` argument to specify the number of GPUs available on your server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a922499-22c2-4702-b1df-0e88dcf27e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 detection/train.py\\\n",
    "    --dataset coco --data-path=/run/determined/workdir/shared_fs/data/xview_dataset/ --model fasterrcnn_resnet50_fpn --epochs 26\\\n",
    "    --lr-steps 16 22 --aspect-ratio-group-factor 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4277f-c47f-4f6b-a8ae-1e551c922947",
   "metadata": {},
   "source": [
    "### 5. Preprocess Satellite Imagery Dataset with Pytorch\n",
    "* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc98c409-336a-4659-a3aa-3fcd65ce0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860049a-7760-4b25-a218-7de7a964c427",
   "metadata": {},
   "source": [
    "### 6. Object Detection on Satellite Imagery with Pytorch (Single GPU)\n",
    "Follow and Run the code to train a Faster RCNN FPN (Resnet50 backbone) that classifies images of clothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb25a7e-99c5-4a62-b500-ab3267cc8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f086cc2-f425-4230-ad62-1cc77b7a3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import python dependencies\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "\n",
    "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import presets\n",
    "import utils\n",
    "from train import *\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5341acc2-cbb6-4104-a43d-c14679534a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output'\n",
    "data_path='/run/determined/workdir/shared_fs/data/xview_dataset/'\n",
    "dataset_name='coco'\n",
    "model='fasterrcnn_resnet50_fpn'\n",
    "device='cuda'\n",
    "batch_size=8\n",
    "epochs=26\n",
    "workers=4\n",
    "lr=0.02\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "lr_scheduler='multisteplr'\n",
    "lr_step_size=8\n",
    "lr_steps=[16, 22]\n",
    "lr_gamma=0.1\n",
    "print_freq=20\n",
    "resume=False\n",
    "start_epoch=0\n",
    "aspect_ratio_group_factor=3\n",
    "rpn_score_thresh=None\n",
    "trainable_backbone_layers=None\n",
    "data_augmentation='hflip'\n",
    "pretrained=True\n",
    "test_only=False\n",
    "sync_bn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e27ee1-fe2e-42d8-92ab-d0668e964fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "loading annotations into memory...\n",
      "Done (t=2.66s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "<torch.utils.data.dataset.Subset object at 0x7f0bf4220e50>\n",
      "61\n",
      "loading annotations into memory...\n",
      "Done (t=1.10s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "Dataset CocoDetection\n",
      "    Number of datapoints: 7166\n",
      "    Root location: /run/determined/workdir/shared_fs/data/xview_dataset/val_sliced_no_neg/val_images_300_02\n",
      "61\n",
      "61\n",
      "Creating data loaders\n",
      "Using [0, 0.5, 0.6299605249474366, 0.7937005259840997, 1.0, 1.2599210498948732, 1.5874010519681994, 2.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [20606]\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "# Data loading code\n",
    "print(\"Loading data\")\n",
    "\n",
    "dataset, num_classes = get_dataset(dataset_name, \"train\", get_transform(True, data_augmentation),\n",
    "                                   data_path)\n",
    "dataset_test, _ = get_dataset(dataset_name, \"val\", get_transform(False, data_augmentation), data_path)\n",
    "print(dataset.num_classes)\n",
    "print(\"Creating data loaders\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(\n",
    "            train_sampler, batch_size, drop_last=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a69314e-d6ec-47ae-99a6-4e29a390a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at one of the images. The following code visualizes the images using the matplotlib library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007b07d6-e668-4ed4-b542-139073b7676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look again at the first ten images, but this time with the class names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6563f0-af36-42f1-8e9a-fe759ac75694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frcnn_model(num_classes):\n",
    "    # load an detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.min_size=800\n",
    "    model.max_size=1333\n",
    "    # RPN parameters\n",
    "    model.rpn_pre_nms_top_n_train=2000\n",
    "    model.rpn_pre_nms_top_n_test=1000\n",
    "    model.rpn_post_nms_top_n_train=2000\n",
    "    model.rpn_post_nms_top_n_test=1000\n",
    "    model.rpn_nms_thresh=0.7\n",
    "    model.rpn_fg_iou_thresh=0.7\n",
    "    model.rpn_bg_iou_thresh=0.3\n",
    "    model.rpn_batch_size_per_image=256\n",
    "    model.rpn_positive_fraction=0.5\n",
    "    model.rpn_score_thresh=0.0\n",
    "    # Box parameters\n",
    "    model.box_score_thresh=0.05\n",
    "    model.box_nms_thresh=0.5\n",
    "    model.box_detections_per_img=100\n",
    "    model.box_fg_iou_thresh=0.5\n",
    "    model.box_bg_iou_thresh=0.5\n",
    "    model.box_batch_size_per_image=512\n",
    "    model.box_positive_fraction=0.25\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9799ae1-d8d8-47e5-a292-e7d0aef1d42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n"
     ]
    }
   ],
   "source": [
    "# Let's build the model:\n",
    "print(\"Creating model\")\n",
    "kwargs = {\n",
    "    \"trainable_backbone_layers\": trainable_backbone_layers\n",
    "}\n",
    "if \"rcnn\" in model:\n",
    "    if rpn_score_thresh is not None:\n",
    "        kwargs[\"rpn_score_thresh\"] = rpn_score_thresh\n",
    "model = build_frcnn_model(dataset.num_classes)\n",
    "_=model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f15cf3-ea3c-4be6-9631-fcb0f24e4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model:\n",
    "# Define loss function, optimizer, and metrics.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = lr_scheduler.lower()\n",
    "if lr_scheduler == 'multisteplr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=lr_steps, \n",
    "                                                        gamma=lr_gamma)\n",
    "elif lr_scheduler == 'cosineannealinglr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "else:\n",
    "    raise RuntimeError(\"Invalid lr scheduler '{}'. Only MultiStepLR and CosineAnnealingLR \"\n",
    "                       \"are supported.\".format(args.lr_scheduler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e308101-e9ff-46d7-b0ca-c5ca32487278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2575]  eta: 2:47:07  lr: 0.000040  loss: 6.9348 (6.9348)  loss_classifier: 3.9348 (3.9348)  loss_box_reg: 0.5733 (0.5733)  loss_objectness: 2.2002 (2.2002)  loss_rpn_box_reg: 0.2265 (0.2265)  time: 3.8942  data: 0.4338  max mem: 7172\n",
      "Epoch: [0]  [  20/2575]  eta: 2:30:39  lr: 0.000440  loss: 3.6758 (3.8337)  loss_classifier: 2.6189 (2.5724)  loss_box_reg: 0.4708 (0.4865)  loss_objectness: 0.2969 (0.6739)  loss_rpn_box_reg: 0.0792 (0.1009)  time: 3.5202  data: 0.0288  max mem: 7454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_dir:\n",
      "File \u001b[0;32m/run/determined/workdir/e2e_blogposts/ngc_blog/detection/engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 31\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:96\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     95\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, features)])\n\u001b[0;32m---> 96\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m     98\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:344\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    343\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features)\n\u001b[0;32m--> 344\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(anchors)\n\u001b[1;32m    347\u001b[0m num_anchors_per_level_shape_tensors \u001b[38;5;241m=\u001b[39m [o[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m objectness]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py:122\u001b[0m, in \u001b[0;36mAnchorGenerator.forward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    120\u001b[0m image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    121\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 122\u001b[0m strides \u001b[38;5;241m=\u001b[39m [[torch\u001b[38;5;241m.\u001b[39mtensor(image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m    123\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grid_sizes]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cell_anchors(dtype, device)\n\u001b[1;32m    125\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_anchors(grid_sizes, strides)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/anchor_utils.py:122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m image_size \u001b[38;5;241m=\u001b[39m image_list\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    121\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, feature_maps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 122\u001b[0m strides \u001b[38;5;241m=\u001b[39m [[\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    123\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m g[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mdevice)] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grid_sizes]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_cell_anchors(dtype, device)\n\u001b[1;32m    125\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_anchors(grid_sizes, strides)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "# Let's train 1 epoch. After every epoch, training time, loss, and accuracy will be displayed.\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq)\n",
    "    lr_scheduler.step()\n",
    "    if output_dir:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'model_{}.pth'.format(epoch)))\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'checkpoint.pth'))\n",
    "\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c14aea-3278-434b-b8b7-f878fd9c0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model performs on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ea014-2a14-488f-bc2f-175e533e6fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
