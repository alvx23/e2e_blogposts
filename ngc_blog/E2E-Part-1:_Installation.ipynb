{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef88a6b-8356-4e68-a929-eb929655b021",
   "metadata": {},
   "source": [
    "# Part 1: E2E Example training object detection model using NVIDIA Pytorch Container from NGC\n",
    " ----\n",
    "\n",
    "Note this Object Detection demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Install the Docker Engine on your system\n",
    "* Pull a Pytorch container from the NGC Catalog using Docker\n",
    "* Run the Pytorch container using Docker\n",
    "* Part 2 : Preprocess the Satellite imagery object detection dataset using the SAHI library\n",
    "* Part 3 : Execute training a object detection on satellite imagery using TensorFlow and Jupyter Notebook\n",
    "* Part 4 : Run inference on a trained object detection model using the SAHI library\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Install the Docker Engine\n",
    "Go to https://docs.docker.com/engine/install/ to install the Docker Engine on your system.\n",
    "\n",
    "\n",
    "### 2. Download the TensorFlow container from the NGC Catalog \n",
    "\n",
    "Once the Docker Engine is installed on your machine, visit https://ngc.nvidia.com/catalog/containers and search for the TensorFlow container. Click on the TensorFlow card and copy the pull command.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/NGC.png\">\n",
    "\n",
    "Open the command line of your machine and past the pull command into your command line. Execute the command to download the container. \n",
    "\n",
    "```$ docker pull nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "    \n",
    "The container starts downloading to your computer. A container image consists of many layers; all of them need to be pulled. \n",
    "\n",
    "### 3. Run the TensorFlow container image\n",
    "\n",
    "Once the container download is completed, run the following code in your command line to run and start the container:\n",
    "\n",
    "```$ docker run -it --gpus all  -p 8888:8888 -v $PWD:/projects --network=host nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline1.png\">\n",
    "\n",
    "### 4. Install Jupyter lab and open a notebook\n",
    "\n",
    "Within the container, run the following commands:\n",
    "\n",
    "```pip install torchvision==0.11.3 jupyterlab```\n",
    "\n",
    "```jupyter lab --ip=0.0.0.0 --port=8888 --allow-root```\n",
    "\n",
    "Open up your favorite browser and enter: http://localhost:8888/?token=*yourtoken*.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline2.png\">\n",
    "\n",
    "You should see the Jupyter Lab application. Click on the plus icon to launch a new Python 3 noteb\n",
    "ook.\n",
    "\n",
    "Follow along with the image classification with the TensorFlow example provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f70686f-3a43-4ede-8526-03684ec6128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Downloading Cython-0.29.33-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycocotools\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from pycocotools) (1.23.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=425644 sha256=ede479a985b076b8d128e2b0704aeac62ebbb4cb010cccacf5b39b4e98db6d68\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o_xoxxl7/wheels/07/42/0a/26d14b3a7343223fcee09c101e73d82fff8bdd1541d85fe033\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: cython, pycocotools\n",
      "Successfully installed cython-0.29.33 pycocotools-2.0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cython pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46ad70-6d9d-41bf-8b69-735c1f34b361",
   "metadata": {},
   "source": [
    "# TLDR; run training job on 8 GPUS\n",
    "The below cell will run a multi-gpu training job. This job will train an object detection model (faster-rcnn) on a dataset of satellite imagery images that contain 61 classes of objects\n",
    "* Change `nproc_per_node` argument to specify the number of GPUs available on your server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a922499-22c2-4702-b1df-0e88dcf27e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 detection/train.py\\\n",
    "    --dataset coco --data-path=/run/determined/workdir/shared_fs/data/xview_dataset/ --model fasterrcnn_resnet50_fpn --epochs 26\\\n",
    "    --lr-steps 16 22 --aspect-ratio-group-factor 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4277f-c47f-4f6b-a8ae-1e551c922947",
   "metadata": {},
   "source": [
    "### 5. Preprocess Satellite Imagery Dataset with Pytorch\n",
    "* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc98c409-336a-4659-a3aa-3fcd65ce0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860049a-7760-4b25-a218-7de7a964c427",
   "metadata": {},
   "source": [
    "### 6. Object Detection on Satellite Imagery with Pytorch (Single GPU)\n",
    "Follow and Run the code to train a Faster RCNN FPN (Resnet50 backbone) that classifies images of clothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb25a7e-99c5-4a62-b500-ab3267cc8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f086cc2-f425-4230-ad62-1cc77b7a3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import python dependencies\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "\n",
    "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import presets\n",
    "import utils\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5341acc2-cbb6-4104-a43d-c14679534a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='.'\n",
    "data_path='/run/determined/workdir/shared_fs/data/xview_dataset/'\n",
    "dataset_name='coco'\n",
    "model='fasterrcnn_resnet50_fpn'\n",
    "device='cuda'\n",
    "batch_size=16\n",
    "epochs=26\n",
    "workers=4\n",
    "lr=0.02\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "lr_scheduler='multisteplr'\n",
    "lr_step_size=8\n",
    "lr_steps=[16, 22]\n",
    "lr_gamma=0.1\n",
    "print_freq=20\n",
    "resume=False\n",
    "start_epoch=0\n",
    "aspect_ratio_group_factor=3\n",
    "rpn_score_thresh=None\n",
    "trainable_backbone_layers=None\n",
    "data_augmentation='hflip'\n",
    "pretrained=True\n",
    "test_only=False\n",
    "sync_bn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f974a81-136d-459c-981f-e57d0b411b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# if output_dir:\n",
    "#         utils.mkdir(output_dir)\n",
    "device = torch.device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e27ee1-fe2e-42d8-92ab-d0668e964fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "loading annotations into memory...\n",
      "Done (t=3.59s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "<torch.utils.data.dataset.Subset object at 0x7fed56aa2f70>\n",
      "61\n",
      "loading annotations into memory...\n",
      "Done (t=1.19s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "Dataset CocoDetection\n",
      "    Number of datapoints: 7166\n",
      "    Root location: /run/determined/workdir/shared_fs/data/xview_dataset/val_sliced_no_neg/val_images_300_02\n",
      "61\n",
      "Creating data loaders\n",
      "Using [0, 0.5, 0.6299605249474366, 0.7937005259840997, 1.0, 1.2599210498948732, 1.5874010519681994, 2.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [20606]\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "# Data loading code\n",
    "print(\"Loading data\")\n",
    "\n",
    "dataset, num_classes = get_dataset(dataset_name, \"train\", get_transform(True, data_augmentation),\n",
    "                                   data_path)\n",
    "dataset_test, _ = get_dataset(dataset_name, \"val\", get_transform(False, data_augmentation), data_path)\n",
    "\n",
    "print(\"Creating data loaders\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(\n",
    "            train_sampler, batch_size, drop_last=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69314e-d6ec-47ae-99a6-4e29a390a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at one of the images. The following code visualizes the images using the matplotlib library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b07d6-e668-4ed4-b542-139073b7676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look again at the first ten images, but this time with the class names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9799ae1-d8d8-47e5-a292-e7d0aef1d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f15cf3-ea3c-4be6-9631-fcb0f24e4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model:\n",
    "# Define loss function, optimizer, and metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e308101-e9ff-46d7-b0ca-c5ca32487278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "# Let's train 1 epoch. After every epoch, training time, loss, and accuracy will be displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c14aea-3278-434b-b8b7-f878fd9c0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model performs on the test data:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
