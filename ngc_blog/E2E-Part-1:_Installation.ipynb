{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef88a6b-8356-4e68-a929-eb929655b021",
   "metadata": {},
   "source": [
    "# End-to-End Example training object detection model using NVIDIA Pytorch Container from NGC\n",
    " ----\n",
    "\n",
    "Note this Object Detection demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Install the Docker Engine on your system\n",
    "* Pull a Pytorch container from the NGC Catalog using Docker\n",
    "* Run the Pytorch container using Docker\n",
    "* Part 2 : Preprocess the Satellite imagery object detection dataset using the SAHI library\n",
    "* Part 3 : Execute training a object detection on satellite imagery using TensorFlow and Jupyter Notebook\n",
    "* Part 4 : Run inference on a trained object detection model using the SAHI library\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Install the Docker Engine\n",
    "Go to https://docs.docker.com/engine/install/ to install the Docker Engine on your system.\n",
    "\n",
    "\n",
    "### 2. Download the TensorFlow container from the NGC Catalog \n",
    "\n",
    "Once the Docker Engine is installed on your machine, visit https://ngc.nvidia.com/catalog/containers and search for the TensorFlow container. Click on the TensorFlow card and copy the pull command.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/NGC.png\">\n",
    "\n",
    "Open the command line of your machine and past the pull command into your command line. Execute the command to download the container. \n",
    "\n",
    "```$ docker pull nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "    \n",
    "The container starts downloading to your computer. A container image consists of many layers; all of them need to be pulled. \n",
    "\n",
    "### 3. Run the TensorFlow container image\n",
    "\n",
    "Once the container download is completed, run the following code in your command line to run and start the container:\n",
    "\n",
    "```$ docker run -it --gpus all  -p 8888:8888 -v $PWD:/projects --network=host nvcr.io/nvidia/pytorch:21.11-py3```\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline1.png\">\n",
    "\n",
    "### 4. Install Jupyter lab and open a notebook\n",
    "\n",
    "Within the container, run the following commands:\n",
    "\n",
    "```pip install torchvision==0.11.3 jupyterlab```\n",
    "\n",
    "```jupyter lab --ip=0.0.0.0 --port=8888 --allow-root```\n",
    "\n",
    "Open up your favorite browser and enter:Â http://localhost:8888/?token=*yourtoken*.\n",
    "UPDATE IMG\n",
    "<img src=\"https://raw.githubusercontent.com/kbojo/images/master/commandline2.png\">\n",
    "\n",
    "You should see the Jupyter Lab application. Click on the plus icon to launch a new Python 3 noteb\n",
    "ook.\n",
    "\n",
    "Follow along with the image classification with the TensorFlow example provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f70686f-3a43-4ede-8526-03684ec6128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install cython pycocotools matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46ad70-6d9d-41bf-8b69-735c1f34b361",
   "metadata": {},
   "source": [
    "# TLDR; run training job on 8 GPUS\n",
    "The below cell will run a multi-gpu training job. This job will train an object detection model (faster-rcnn) on a dataset of satellite imagery images that contain 61 classes of objects\n",
    "* Change `nproc_per_node` argument to specify the number of GPUs available on your server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a922499-22c2-4702-b1df-0e88dcf27e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 detection/train.py\\\n",
    "    --dataset coco --data-path=/run/determined/workdir/shared_fs/data/xview_dataset/ --model fasterrcnn_resnet50_fpn --epochs 26\\\n",
    "    --lr-steps 16 22 --aspect-ratio-group-factor 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4277f-c47f-4f6b-a8ae-1e551c922947",
   "metadata": {},
   "source": [
    "### 5. Preprocess Satellite Imagery Dataset with Pytorch\n",
    "* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc98c409-336a-4659-a3aa-3fcd65ce0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860049a-7760-4b25-a218-7de7a964c427",
   "metadata": {},
   "source": [
    "### 6. Object Detection on Satellite Imagery with Pytorch (Single GPU)\n",
    "Follow and Run the code to train a Faster RCNN FPN (Resnet50 backbone) that classifies images of clothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb25a7e-99c5-4a62-b500-ab3267cc8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f086cc2-f425-4230-ad62-1cc77b7a3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python dependencies\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco, get_coco_kp\n",
    "\n",
    "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import presets\n",
    "import utils\n",
    "from train import *\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5341acc2-cbb6-4104-a43d-c14679534a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='output'\n",
    "data_path='/run/determined/workdir/shared_fs/data/xview_dataset/'\n",
    "dataset_name='coco'\n",
    "model_name='fasterrcnn_resnet50_fpn'\n",
    "device='cuda'\n",
    "batch_size=8\n",
    "epochs=26\n",
    "workers=4\n",
    "lr=0.02\n",
    "momentum=0.9\n",
    "weight_decay=1e-4\n",
    "lr_scheduler='multisteplr'\n",
    "lr_step_size=8\n",
    "lr_steps=[16, 22]\n",
    "lr_gamma=0.1\n",
    "print_freq=20\n",
    "resume=False\n",
    "start_epoch=0\n",
    "aspect_ratio_group_factor=3\n",
    "rpn_score_thresh=None\n",
    "trainable_backbone_layers=None\n",
    "data_augmentation='hflip'\n",
    "pretrained=True\n",
    "test_only=False\n",
    "sync_bn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e27ee1-fe2e-42d8-92ab-d0668e964fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "loading annotations into memory...\n",
      "Done (t=2.65s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "<torch.utils.data.dataset.Subset object at 0x7f5420123070>\n",
      "61\n",
      "loading annotations into memory...\n",
      "Done (t=1.07s)\n",
      "creating index...\n",
      "index created!\n",
      "self.catIdtoCls:  {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 12, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 26, 26: 27, 27: 28, 28: 29, 29: 30, 30: 31, 31: 32, 32: 33, 33: 34, 34: 35, 35: 36, 36: 37, 37: 38, 38: 39, 39: 40, 40: 41, 41: 42, 42: 43, 43: 44, 44: 45, 45: 46, 46: 47, 47: 48, 48: 49, 49: 50, 50: 51, 51: 52, 52: 53, 53: 54, 54: 55, 55: 56, 56: 57, 57: 58, 58: 59, 59: 60}\n",
      "self.num_classes:  61\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59}\n",
      "Dataset CocoDetection\n",
      "    Number of datapoints: 7166\n",
      "    Root location: /run/determined/workdir/shared_fs/data/xview_dataset/val_sliced_no_neg/val_images_300_02\n",
      "61\n",
      "61\n",
      "Creating data loaders\n",
      "Using [0, 0.5, 0.6299605249474366, 0.7937005259840997, 1.0, 1.2599210498948732, 1.5874010519681994, 2.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [20606]\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset.\n",
    "# Data loading code\n",
    "print(\"Loading data\")\n",
    "\n",
    "dataset, num_classes = get_dataset(dataset_name, \"train\", get_transform(True, data_augmentation),\n",
    "                                   data_path)\n",
    "dataset_test, _ = get_dataset(dataset_name, \"val\", get_transform(False, data_augmentation), data_path)\n",
    "print(dataset.num_classes)\n",
    "print(\"Creating data loaders\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(\n",
    "            train_sampler, batch_size, drop_last=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_sampler=train_batch_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler, num_workers=workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a69314e-d6ec-47ae-99a6-4e29a390a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at one of the images. The following code visualizes the images using the matplotlib library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007b07d6-e668-4ed4-b542-139073b7676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look again at the first ten images, but this time with the class names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d6563f0-af36-42f1-8e9a-fe759ac75694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frcnn_model(num_classes):\n",
    "    # load an detection model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.min_size=800\n",
    "    model.max_size=1333\n",
    "    # RPN parameters\n",
    "    model.rpn_pre_nms_top_n_train=2000\n",
    "    model.rpn_pre_nms_top_n_test=1000\n",
    "    model.rpn_post_nms_top_n_train=2000\n",
    "    model.rpn_post_nms_top_n_test=1000\n",
    "    model.rpn_nms_thresh=0.7\n",
    "    model.rpn_fg_iou_thresh=0.7\n",
    "    model.rpn_bg_iou_thresh=0.3\n",
    "    model.rpn_batch_size_per_image=256\n",
    "    model.rpn_positive_fraction=0.5\n",
    "    model.rpn_score_thresh=0.0\n",
    "    # Box parameters\n",
    "    model.box_score_thresh=0.0\n",
    "    model.box_nms_thresh=0.5\n",
    "    model.box_detections_per_img=100\n",
    "    model.box_fg_iou_thresh=0.5\n",
    "    model.box_bg_iou_thresh=0.5\n",
    "    model.box_batch_size_per_image=512\n",
    "    model.box_positive_fraction=0.25\n",
    "    return model\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_determined_state_dict(ckpt):\n",
    "    '''\n",
    "    Removes module from state dict keys as determined saves model in DataParallel format:\n",
    "    https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "    '''\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in ckpt['models_state_dict'][0].items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9799ae1-d8d8-47e5-a292-e7d0aef1d42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n",
      "Number of classes:  61\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.weight\", \"backbone.fpn.inner_blocks.0.bias\", \"backbone.fpn.inner_blocks.1.weight\", \"backbone.fpn.inner_blocks.1.bias\", \"backbone.fpn.inner_blocks.2.weight\", \"backbone.fpn.inner_blocks.2.bias\", \"backbone.fpn.inner_blocks.3.weight\", \"backbone.fpn.inner_blocks.3.bias\", \"backbone.fpn.layer_blocks.0.weight\", \"backbone.fpn.layer_blocks.0.bias\", \"backbone.fpn.layer_blocks.1.weight\", \"backbone.fpn.layer_blocks.1.bias\", \"backbone.fpn.layer_blocks.2.weight\", \"backbone.fpn.layer_blocks.2.bias\", \"backbone.fpn.layer_blocks.3.weight\", \"backbone.fpn.layer_blocks.3.bias\", \"rpn.head.conv.weight\", \"rpn.head.conv.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m trained_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/run/determined/workdir/checkpoints/5bf118a9-6a5c-459f-ba3d-df9afd94d540/state_dict.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m load_determined_state_dict(torch\u001b[38;5;241m.\u001b[39mload(trained_model_dir,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.weight\", \"backbone.fpn.inner_blocks.0.bias\", \"backbone.fpn.inner_blocks.1.weight\", \"backbone.fpn.inner_blocks.1.bias\", \"backbone.fpn.inner_blocks.2.weight\", \"backbone.fpn.inner_blocks.2.bias\", \"backbone.fpn.inner_blocks.3.weight\", \"backbone.fpn.inner_blocks.3.bias\", \"backbone.fpn.layer_blocks.0.weight\", \"backbone.fpn.layer_blocks.0.bias\", \"backbone.fpn.layer_blocks.1.weight\", \"backbone.fpn.layer_blocks.1.bias\", \"backbone.fpn.layer_blocks.2.weight\", \"backbone.fpn.layer_blocks.2.bias\", \"backbone.fpn.layer_blocks.3.weight\", \"backbone.fpn.layer_blocks.3.bias\", \"rpn.head.conv.weight\", \"rpn.head.conv.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.weight\", \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.weight\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.weight\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.weight\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.weight\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.weight\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.weight\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.weight\", \"backbone.fpn.layer_blocks.3.0.bias\", \"rpn.head.conv.0.0.weight\", \"rpn.head.conv.0.0.bias\". "
     ]
    }
   ],
   "source": [
    "# Let's build the model:\n",
    "print(\"Creating model\")\n",
    "kwargs = {\n",
    "    \"trainable_backbone_layers\": trainable_backbone_layers\n",
    "}\n",
    "if \"rcnn\" in model_name:\n",
    "    if rpn_score_thresh is not None:\n",
    "        kwargs[\"rpn_score_thresh\"] = rpn_score_thresh\n",
    "print(\"Number of classes: \",dataset.num_classes)\n",
    "model = build_frcnn_model(num_classes=dataset.num_classes)\n",
    "_=model.to(device)\n",
    "# trained_model_dir = '/run/determined/workdir/checkpoints/5bf118a9-6a5c-459f-ba3d-df9afd94d540/state_dict.pth'\n",
    "# ckpt = load_determined_state_dict(torch.load(trained_model_dir,map_location='cpu'))\n",
    "# model.load_state_dict(ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f15cf3-ea3c-4be6-9631-fcb0f24e4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model:\n",
    "# Define loss function, optimizer, and metrics.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = lr_scheduler.lower()\n",
    "if lr_scheduler == 'multisteplr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                        milestones=lr_steps, \n",
    "                                                        gamma=lr_gamma)\n",
    "elif lr_scheduler == 'cosineannealinglr':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "else:\n",
    "    raise RuntimeError(\"Invalid lr scheduler '{}'. Only MultiStepLR and CosineAnnealingLR \"\n",
    "                       \"are supported.\".format(args.lr_scheduler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e308101-e9ff-46d7-b0ca-c5ca32487278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "# Let's train 1 epoch. After every epoch, training time, loss, and accuracy will be displayed.\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq)\n",
    "    lr_scheduler.step()\n",
    "    if output_dir:\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'model_{}.pth'.format(epoch)))\n",
    "        utils.save_on_master(\n",
    "            checkpoint,\n",
    "            os.path.join(output_dir, 'checkpoint.pth'))\n",
    "\n",
    "    # evaluate after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c14aea-3278-434b-b8b7-f878fd9c0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the model performs on the test data:\n",
    "def to_float_tensor(img):\n",
    "    \"\"\"\n",
    "    Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W).\n",
    "    Args:\n",
    "        img: np.ndarray\n",
    "    Returns:\n",
    "        torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = torch.from_numpy(np.array(img)).float()\n",
    "    if img.max() > 1:\n",
    "        img /= 255\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a83ea014-2a14-488f-bc2f-175e533e6fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.weight\", \"backbone.fpn.inner_blocks.0.bias\", \"backbone.fpn.inner_blocks.1.weight\", \"backbone.fpn.inner_blocks.1.bias\", \"backbone.fpn.inner_blocks.2.weight\", \"backbone.fpn.inner_blocks.2.bias\", \"backbone.fpn.inner_blocks.3.weight\", \"backbone.fpn.inner_blocks.3.bias\", \"backbone.fpn.layer_blocks.0.weight\", \"backbone.fpn.layer_blocks.0.bias\", \"backbone.fpn.layer_blocks.1.weight\", \"backbone.fpn.layer_blocks.1.bias\", \"backbone.fpn.layer_blocks.2.weight\", \"backbone.fpn.layer_blocks.2.bias\", \"backbone.fpn.layer_blocks.3.weight\", \"backbone.fpn.layer_blocks.3.bias\", \"rpn.head.conv.weight\", \"rpn.head.conv.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"models_state_dict\", \"optimizers_state_dict\", \"lr_schedulers_state_dict\", \"callbacks\", \"rng_state\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trained_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/run/determined/workdir/checkpoints/5bf118a9-6a5c-459f-ba3d-df9afd94d540/state_dict.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", \"backbone.body.bn1.weight\", \"backbone.body.bn1.bias\", \"backbone.body.bn1.running_mean\", \"backbone.body.bn1.running_var\", \"backbone.body.layer1.0.conv1.weight\", \"backbone.body.layer1.0.bn1.weight\", \"backbone.body.layer1.0.bn1.bias\", \"backbone.body.layer1.0.bn1.running_mean\", \"backbone.body.layer1.0.bn1.running_var\", \"backbone.body.layer1.0.conv2.weight\", \"backbone.body.layer1.0.bn2.weight\", \"backbone.body.layer1.0.bn2.bias\", \"backbone.body.layer1.0.bn2.running_mean\", \"backbone.body.layer1.0.bn2.running_var\", \"backbone.body.layer1.0.conv3.weight\", \"backbone.body.layer1.0.bn3.weight\", \"backbone.body.layer1.0.bn3.bias\", \"backbone.body.layer1.0.bn3.running_mean\", \"backbone.body.layer1.0.bn3.running_var\", \"backbone.body.layer1.0.downsample.0.weight\", \"backbone.body.layer1.0.downsample.1.weight\", \"backbone.body.layer1.0.downsample.1.bias\", \"backbone.body.layer1.0.downsample.1.running_mean\", \"backbone.body.layer1.0.downsample.1.running_var\", \"backbone.body.layer1.1.conv1.weight\", \"backbone.body.layer1.1.bn1.weight\", \"backbone.body.layer1.1.bn1.bias\", \"backbone.body.layer1.1.bn1.running_mean\", \"backbone.body.layer1.1.bn1.running_var\", \"backbone.body.layer1.1.conv2.weight\", \"backbone.body.layer1.1.bn2.weight\", \"backbone.body.layer1.1.bn2.bias\", \"backbone.body.layer1.1.bn2.running_mean\", \"backbone.body.layer1.1.bn2.running_var\", \"backbone.body.layer1.1.conv3.weight\", \"backbone.body.layer1.1.bn3.weight\", \"backbone.body.layer1.1.bn3.bias\", \"backbone.body.layer1.1.bn3.running_mean\", \"backbone.body.layer1.1.bn3.running_var\", \"backbone.body.layer1.2.conv1.weight\", \"backbone.body.layer1.2.bn1.weight\", \"backbone.body.layer1.2.bn1.bias\", \"backbone.body.layer1.2.bn1.running_mean\", \"backbone.body.layer1.2.bn1.running_var\", \"backbone.body.layer1.2.conv2.weight\", \"backbone.body.layer1.2.bn2.weight\", \"backbone.body.layer1.2.bn2.bias\", \"backbone.body.layer1.2.bn2.running_mean\", \"backbone.body.layer1.2.bn2.running_var\", \"backbone.body.layer1.2.conv3.weight\", \"backbone.body.layer1.2.bn3.weight\", \"backbone.body.layer1.2.bn3.bias\", \"backbone.body.layer1.2.bn3.running_mean\", \"backbone.body.layer1.2.bn3.running_var\", \"backbone.body.layer2.0.conv1.weight\", \"backbone.body.layer2.0.bn1.weight\", \"backbone.body.layer2.0.bn1.bias\", \"backbone.body.layer2.0.bn1.running_mean\", \"backbone.body.layer2.0.bn1.running_var\", \"backbone.body.layer2.0.conv2.weight\", \"backbone.body.layer2.0.bn2.weight\", \"backbone.body.layer2.0.bn2.bias\", \"backbone.body.layer2.0.bn2.running_mean\", \"backbone.body.layer2.0.bn2.running_var\", \"backbone.body.layer2.0.conv3.weight\", \"backbone.body.layer2.0.bn3.weight\", \"backbone.body.layer2.0.bn3.bias\", \"backbone.body.layer2.0.bn3.running_mean\", \"backbone.body.layer2.0.bn3.running_var\", \"backbone.body.layer2.0.downsample.0.weight\", \"backbone.body.layer2.0.downsample.1.weight\", \"backbone.body.layer2.0.downsample.1.bias\", \"backbone.body.layer2.0.downsample.1.running_mean\", \"backbone.body.layer2.0.downsample.1.running_var\", \"backbone.body.layer2.1.conv1.weight\", \"backbone.body.layer2.1.bn1.weight\", \"backbone.body.layer2.1.bn1.bias\", \"backbone.body.layer2.1.bn1.running_mean\", \"backbone.body.layer2.1.bn1.running_var\", \"backbone.body.layer2.1.conv2.weight\", \"backbone.body.layer2.1.bn2.weight\", \"backbone.body.layer2.1.bn2.bias\", \"backbone.body.layer2.1.bn2.running_mean\", \"backbone.body.layer2.1.bn2.running_var\", \"backbone.body.layer2.1.conv3.weight\", \"backbone.body.layer2.1.bn3.weight\", \"backbone.body.layer2.1.bn3.bias\", \"backbone.body.layer2.1.bn3.running_mean\", \"backbone.body.layer2.1.bn3.running_var\", \"backbone.body.layer2.2.conv1.weight\", \"backbone.body.layer2.2.bn1.weight\", \"backbone.body.layer2.2.bn1.bias\", \"backbone.body.layer2.2.bn1.running_mean\", \"backbone.body.layer2.2.bn1.running_var\", \"backbone.body.layer2.2.conv2.weight\", \"backbone.body.layer2.2.bn2.weight\", \"backbone.body.layer2.2.bn2.bias\", \"backbone.body.layer2.2.bn2.running_mean\", \"backbone.body.layer2.2.bn2.running_var\", \"backbone.body.layer2.2.conv3.weight\", \"backbone.body.layer2.2.bn3.weight\", \"backbone.body.layer2.2.bn3.bias\", \"backbone.body.layer2.2.bn3.running_mean\", \"backbone.body.layer2.2.bn3.running_var\", \"backbone.body.layer2.3.conv1.weight\", \"backbone.body.layer2.3.bn1.weight\", \"backbone.body.layer2.3.bn1.bias\", \"backbone.body.layer2.3.bn1.running_mean\", \"backbone.body.layer2.3.bn1.running_var\", \"backbone.body.layer2.3.conv2.weight\", \"backbone.body.layer2.3.bn2.weight\", \"backbone.body.layer2.3.bn2.bias\", \"backbone.body.layer2.3.bn2.running_mean\", \"backbone.body.layer2.3.bn2.running_var\", \"backbone.body.layer2.3.conv3.weight\", \"backbone.body.layer2.3.bn3.weight\", \"backbone.body.layer2.3.bn3.bias\", \"backbone.body.layer2.3.bn3.running_mean\", \"backbone.body.layer2.3.bn3.running_var\", \"backbone.body.layer3.0.conv1.weight\", \"backbone.body.layer3.0.bn1.weight\", \"backbone.body.layer3.0.bn1.bias\", \"backbone.body.layer3.0.bn1.running_mean\", \"backbone.body.layer3.0.bn1.running_var\", \"backbone.body.layer3.0.conv2.weight\", \"backbone.body.layer3.0.bn2.weight\", \"backbone.body.layer3.0.bn2.bias\", \"backbone.body.layer3.0.bn2.running_mean\", \"backbone.body.layer3.0.bn2.running_var\", \"backbone.body.layer3.0.conv3.weight\", \"backbone.body.layer3.0.bn3.weight\", \"backbone.body.layer3.0.bn3.bias\", \"backbone.body.layer3.0.bn3.running_mean\", \"backbone.body.layer3.0.bn3.running_var\", \"backbone.body.layer3.0.downsample.0.weight\", \"backbone.body.layer3.0.downsample.1.weight\", \"backbone.body.layer3.0.downsample.1.bias\", \"backbone.body.layer3.0.downsample.1.running_mean\", \"backbone.body.layer3.0.downsample.1.running_var\", \"backbone.body.layer3.1.conv1.weight\", \"backbone.body.layer3.1.bn1.weight\", \"backbone.body.layer3.1.bn1.bias\", \"backbone.body.layer3.1.bn1.running_mean\", \"backbone.body.layer3.1.bn1.running_var\", \"backbone.body.layer3.1.conv2.weight\", \"backbone.body.layer3.1.bn2.weight\", \"backbone.body.layer3.1.bn2.bias\", \"backbone.body.layer3.1.bn2.running_mean\", \"backbone.body.layer3.1.bn2.running_var\", \"backbone.body.layer3.1.conv3.weight\", \"backbone.body.layer3.1.bn3.weight\", \"backbone.body.layer3.1.bn3.bias\", \"backbone.body.layer3.1.bn3.running_mean\", \"backbone.body.layer3.1.bn3.running_var\", \"backbone.body.layer3.2.conv1.weight\", \"backbone.body.layer3.2.bn1.weight\", \"backbone.body.layer3.2.bn1.bias\", \"backbone.body.layer3.2.bn1.running_mean\", \"backbone.body.layer3.2.bn1.running_var\", \"backbone.body.layer3.2.conv2.weight\", \"backbone.body.layer3.2.bn2.weight\", \"backbone.body.layer3.2.bn2.bias\", \"backbone.body.layer3.2.bn2.running_mean\", \"backbone.body.layer3.2.bn2.running_var\", \"backbone.body.layer3.2.conv3.weight\", \"backbone.body.layer3.2.bn3.weight\", \"backbone.body.layer3.2.bn3.bias\", \"backbone.body.layer3.2.bn3.running_mean\", \"backbone.body.layer3.2.bn3.running_var\", \"backbone.body.layer3.3.conv1.weight\", \"backbone.body.layer3.3.bn1.weight\", \"backbone.body.layer3.3.bn1.bias\", \"backbone.body.layer3.3.bn1.running_mean\", \"backbone.body.layer3.3.bn1.running_var\", \"backbone.body.layer3.3.conv2.weight\", \"backbone.body.layer3.3.bn2.weight\", \"backbone.body.layer3.3.bn2.bias\", \"backbone.body.layer3.3.bn2.running_mean\", \"backbone.body.layer3.3.bn2.running_var\", \"backbone.body.layer3.3.conv3.weight\", \"backbone.body.layer3.3.bn3.weight\", \"backbone.body.layer3.3.bn3.bias\", \"backbone.body.layer3.3.bn3.running_mean\", \"backbone.body.layer3.3.bn3.running_var\", \"backbone.body.layer3.4.conv1.weight\", \"backbone.body.layer3.4.bn1.weight\", \"backbone.body.layer3.4.bn1.bias\", \"backbone.body.layer3.4.bn1.running_mean\", \"backbone.body.layer3.4.bn1.running_var\", \"backbone.body.layer3.4.conv2.weight\", \"backbone.body.layer3.4.bn2.weight\", \"backbone.body.layer3.4.bn2.bias\", \"backbone.body.layer3.4.bn2.running_mean\", \"backbone.body.layer3.4.bn2.running_var\", \"backbone.body.layer3.4.conv3.weight\", \"backbone.body.layer3.4.bn3.weight\", \"backbone.body.layer3.4.bn3.bias\", \"backbone.body.layer3.4.bn3.running_mean\", \"backbone.body.layer3.4.bn3.running_var\", \"backbone.body.layer3.5.conv1.weight\", \"backbone.body.layer3.5.bn1.weight\", \"backbone.body.layer3.5.bn1.bias\", \"backbone.body.layer3.5.bn1.running_mean\", \"backbone.body.layer3.5.bn1.running_var\", \"backbone.body.layer3.5.conv2.weight\", \"backbone.body.layer3.5.bn2.weight\", \"backbone.body.layer3.5.bn2.bias\", \"backbone.body.layer3.5.bn2.running_mean\", \"backbone.body.layer3.5.bn2.running_var\", \"backbone.body.layer3.5.conv3.weight\", \"backbone.body.layer3.5.bn3.weight\", \"backbone.body.layer3.5.bn3.bias\", \"backbone.body.layer3.5.bn3.running_mean\", \"backbone.body.layer3.5.bn3.running_var\", \"backbone.body.layer4.0.conv1.weight\", \"backbone.body.layer4.0.bn1.weight\", \"backbone.body.layer4.0.bn1.bias\", \"backbone.body.layer4.0.bn1.running_mean\", \"backbone.body.layer4.0.bn1.running_var\", \"backbone.body.layer4.0.conv2.weight\", \"backbone.body.layer4.0.bn2.weight\", \"backbone.body.layer4.0.bn2.bias\", \"backbone.body.layer4.0.bn2.running_mean\", \"backbone.body.layer4.0.bn2.running_var\", \"backbone.body.layer4.0.conv3.weight\", \"backbone.body.layer4.0.bn3.weight\", \"backbone.body.layer4.0.bn3.bias\", \"backbone.body.layer4.0.bn3.running_mean\", \"backbone.body.layer4.0.bn3.running_var\", \"backbone.body.layer4.0.downsample.0.weight\", \"backbone.body.layer4.0.downsample.1.weight\", \"backbone.body.layer4.0.downsample.1.bias\", \"backbone.body.layer4.0.downsample.1.running_mean\", \"backbone.body.layer4.0.downsample.1.running_var\", \"backbone.body.layer4.1.conv1.weight\", \"backbone.body.layer4.1.bn1.weight\", \"backbone.body.layer4.1.bn1.bias\", \"backbone.body.layer4.1.bn1.running_mean\", \"backbone.body.layer4.1.bn1.running_var\", \"backbone.body.layer4.1.conv2.weight\", \"backbone.body.layer4.1.bn2.weight\", \"backbone.body.layer4.1.bn2.bias\", \"backbone.body.layer4.1.bn2.running_mean\", \"backbone.body.layer4.1.bn2.running_var\", \"backbone.body.layer4.1.conv3.weight\", \"backbone.body.layer4.1.bn3.weight\", \"backbone.body.layer4.1.bn3.bias\", \"backbone.body.layer4.1.bn3.running_mean\", \"backbone.body.layer4.1.bn3.running_var\", \"backbone.body.layer4.2.conv1.weight\", \"backbone.body.layer4.2.bn1.weight\", \"backbone.body.layer4.2.bn1.bias\", \"backbone.body.layer4.2.bn1.running_mean\", \"backbone.body.layer4.2.bn1.running_var\", \"backbone.body.layer4.2.conv2.weight\", \"backbone.body.layer4.2.bn2.weight\", \"backbone.body.layer4.2.bn2.bias\", \"backbone.body.layer4.2.bn2.running_mean\", \"backbone.body.layer4.2.bn2.running_var\", \"backbone.body.layer4.2.conv3.weight\", \"backbone.body.layer4.2.bn3.weight\", \"backbone.body.layer4.2.bn3.bias\", \"backbone.body.layer4.2.bn3.running_mean\", \"backbone.body.layer4.2.bn3.running_var\", \"backbone.fpn.inner_blocks.0.weight\", \"backbone.fpn.inner_blocks.0.bias\", \"backbone.fpn.inner_blocks.1.weight\", \"backbone.fpn.inner_blocks.1.bias\", \"backbone.fpn.inner_blocks.2.weight\", \"backbone.fpn.inner_blocks.2.bias\", \"backbone.fpn.inner_blocks.3.weight\", \"backbone.fpn.inner_blocks.3.bias\", \"backbone.fpn.layer_blocks.0.weight\", \"backbone.fpn.layer_blocks.0.bias\", \"backbone.fpn.layer_blocks.1.weight\", \"backbone.fpn.layer_blocks.1.bias\", \"backbone.fpn.layer_blocks.2.weight\", \"backbone.fpn.layer_blocks.2.bias\", \"backbone.fpn.layer_blocks.3.weight\", \"backbone.fpn.layer_blocks.3.bias\", \"rpn.head.conv.weight\", \"rpn.head.conv.bias\", \"rpn.head.cls_logits.weight\", \"rpn.head.cls_logits.bias\", \"rpn.head.bbox_pred.weight\", \"rpn.head.bbox_pred.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\", \"roi_heads.box_predictor.cls_score.weight\", \"roi_heads.box_predictor.cls_score.bias\", \"roi_heads.box_predictor.bbox_pred.weight\", \"roi_heads.box_predictor.bbox_pred.bias\". \n\tUnexpected key(s) in state_dict: \"models_state_dict\", \"optimizers_state_dict\", \"lr_schedulers_state_dict\", \"callbacks\", \"rng_state\". "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c3dec-0888-4b33-94ec-14d1ea0c79c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
