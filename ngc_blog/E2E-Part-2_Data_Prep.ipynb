{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b15f61-d287-42fd-b6fd-5be4f395a645",
   "metadata": {},
   "source": [
    "# Part 2: Data Preparation\n",
    " ----\n",
    "\n",
    "Note this Demo is based on https://github.com/pytorch/vision/tree/v0.11.3\n",
    "\n",
    "This notebook walks you each step to train a model using containers from the NGC Catalog. We chose the GPU optimized Pytorch container as an example. The basics of working with docker containers apply to all NGC containers.\n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Download the Xview Dataset\n",
    "* How to convert labels to coco format\n",
    "* How to conduct the preprocessing step ,Tiling: slicing large satellite imagery into chunks \n",
    "* How to upload to s3 bucket to support distributed training\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2. Download the TensorFlow container from the NGC Catalog \n",
    "\n",
    "Once the Docker Engine is installed on your machine, visit https://ngc.nvidia.com/catalog/containers and search for the TensorFlow container. Click on the TensorFlow card and copy the pull command.\n",
    "UPDATE IMG\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7579a1-a65f-4811-a75e-0ddf0464762c",
   "metadata": {},
   "source": [
    "# 1. Download the Xview Dataset\n",
    "The dataset we will be using is from the DIUx xView 2018 Challenge https://challenge.xviewdataset.org by U.S. National Geospatial-Intelligence Agency (NGA). You will need to create an account at https://challenge.xviewdataset.org/welcome, agree to the terms and conditions, and download the dataset manually.\n",
    "\n",
    "You can download the dataset at the url https://challenge.xviewdataset.org/data-download\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f42447-e4ed-4892-8396-341a7a1fb262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sahi in /opt/conda/lib/python3.8/site-packages (0.11.13)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m153.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8.2.0 in /opt/conda/lib/python3.8/site-packages (from sahi) (9.4.0)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.8/site-packages (from sahi) (0.5.0)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from sahi) (2.0.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from sahi) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.8/site-packages (from sahi) (4.65.0)\n",
      "Requirement already satisfied: opencv-python>=4.2.0.32 in /opt/conda/lib/python3.8/site-packages (from sahi) (4.7.0.72)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from sahi) (2.28.1)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.8/site-packages (from sahi) (3.1.10)\n",
      "Requirement already satisfied: pybboxes==0.1.6 in /opt/conda/lib/python3.8/site-packages (from sahi) (0.1.6)\n",
      "Requirement already satisfied: click==8.0.4 in /opt/conda/lib/python3.8/site-packages (from sahi) (8.0.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from pybboxes==0.1.6->sahi) (1.24.2)\n",
      "Collecting networkx>=2.8\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m191.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.9.2,>=1.8\n",
      "  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m161.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio>=2.4.1\n",
      "  Downloading imageio-2.27.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m172.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2023.3.21-py3-none-any.whl (218 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m188.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (23.0)\n",
      "Collecting lazy_loader>=0.1\n",
      "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from fire->sahi) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from fire->sahi) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->sahi) (1.26.14)\n",
      "Installing collected packages: tifffile, scipy, PyWavelets, networkx, lazy_loader, imageio, scikit-image\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.27.0 lazy_loader-0.2 networkx-3.1 scikit-image-0.20.0 scipy-1.9.1 tifffile-2023.3.21\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# run pip install to get the SAHI library\n",
    "!pip install sahi scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece66b8d-2d96-4d00-8fba-468121245e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to download train images with wget command, you will need to update the url as the token is expired\"\n",
    "!wget -O train_images.tgz \"https://d307kc0mrhucc3.cloudfront.net/train_images.tgz?Expires=1680826141&Signature=par-WmB2Ffj-BZUoVkApKYADOMnqEQMWo7ZalDo47UqhPvy0nDge6pTUaEYH8F7xSR8nJKb3fFHdfqFea9Jua5LgqTa1sp5Ekaw8FloYIJIFvv-S0OxA-5VRpyYNLiKNjIg4uxykSKMYPj3xTq8YicBZNdnrXzafsRxmeQmcbiSqGR~8Jf1PndguouuNa4TV0D4iBtKqF0G6phgNCCF3ofGXO6YwLjjaVyKVsyLcvuQ2xj4KGKJM0AP2VJA43XjlgGJEuNzh1LubPocN8OCUlbm~jnUxq1N0iWDBFfxdW2JhCFN-iajIsgWEzQd0SMxB7bpyQhfWNEFHwNcA71uwKQ__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240220f4-6749-406c-9055-21df206e084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to download train images with wget command, you will need to update the url as the token is expired\"\n",
    "!wget -O train_labels.tgz \"https://d307kc0mrhucc3.cloudfront.net/train_labels.tgz?Expires=1680826141&Signature=ToOHVlmZq6tjN0La0wYL9~DEeaf9HK1F0KB8yy4Izk020HJemSDzakYmhCF3CsXJ3ns-KrZ4Vfws6mIlmfkk9l0FvVByQC94MB618CaRBytbCkO69ONUFAt0OzNUR14DB9cCM6Q3VJ9dHcUw-fAr~D2yHeK3mhnDSNbCQAqOaKoYQlfLxbAdTrMfU8KL6z3vAD6hC0ofa6QtlSxbhJgXupfw7nzgNmtrhF2Q6xX7gDSoi6~7OLu7bisGlA8sJuzPCVONWl5zxwK~ZPOgJsF6UAckdKsd2V-4IX4cWQlZYUf7FkKG3ccT8XFmeHY-9dBfX2AgfFH84p5P~nrPA41R-Q__&Key-Pair-Id=APKAIKGDJB5C3XUL2DXQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddbd7b-6981-4e17-a326-14d06f10814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip images and labels from /run/determined/workdir directory \n",
    "!tar -xvf train_images.tgz -C e2e_blogposts/ngc_blog/xview_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f95e28-e012-4c33-ae7e-c981c058681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip labels from /run/determined/workdir directory \n",
    "!tar -xvf train_labels.tgz -C e2e_blogposts/ngc_blog/xview_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25f5a5-fcc4-46fc-9920-d96a43f131bc",
   "metadata": {},
   "source": [
    "# Convert TIF to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f968fa-e816-4090-bfc9-17dd443f47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renaming bad named files...\n",
      "[PosixPath('100.tif'), PosixPath('109.tif'), PosixPath('102.tif')]\n",
      "100%|███████████████████████████████████████| 846/846 [1:03:55<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Here loop through all the images and convert them to RGB, this is important for tiling the images and training with pytorch\n",
    "# will take about an hour to complete\n",
    "!python data_utils/tif_2_rgb.py --input_dir xview_dataset/train_images \\\n",
    "  --out_dir xview_dataset/train_images_rgb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27421724-87f5-4dfe-8b3b-72fe927e285c",
   "metadata": {},
   "source": [
    "# 2. How to convert labels to coco format\n",
    "Here we run a script to convert the dataset labels from .geojson format to COCO format. More details on the COCO format here: \n",
    "\n",
    "The result will be two files (in COCO formal) generated `train.json` and `val.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8d36a5-a465-4864-acc4-137ec74755af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(category_id_remapping='data_utils/category_id_mapping.json', output_dir='xview_dataset/', train_geojson_path='xview_dataset/xView_train.geojson', train_images_dir='xview_dataset/train_images/', train_images_dir_rgb='xview_dataset/train_images_rgb/', train_split_rate=0.75, xview_class_labels='data_utils/xview_class_labels.txt')\n",
      "5.tif:  True\n",
      "Parsing xView data: 100%|████████████| 601937/601937 [00:14<00:00, 42033.53it/s]\n",
      "chips:  ['2355.png' '2355.png' '2355.png' ... '389.png' '389.png' '389.png']\n",
      "Converting xView data into COCO format: 100%|█| 846/846 [01:12<00:00, 11.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# make sure train_images_dir is pointing to the .tif images\n",
    "!python data_utils/convert_geojson_to_coco.py --train_images_dir xview_dataset/train_images/ \\\n",
    "  --train_images_dir_rgb xview_dataset/train_images_rgb/ \\\n",
    "  --train_geojson_path xview_dataset/xView_train.geojson \\\n",
    "  --output_dir xview_dataset/ \\\n",
    "  --train_split_rate 0.75 \\\n",
    "  --category_id_remapping data_utils/category_id_mapping.json \\\n",
    "  --xview_class_labels data_utils/xview_class_labels.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4367d-f7da-45e3-a501-1b922f7f10f8",
   "metadata": {},
   "source": [
    "# 3. Slicing/Tiling the Dataset\n",
    "Here we are using the SAHI library to slice our large satellite images. Satellite images can be up to 50k^2 pixels in size, which wouldnt fit in GPU memory. We alleviate this problem by slicing the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bed6a0-bdad-4874-8236-0171d1e0d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing step is starting...\n",
      "indexing coco dataset annotations...\n",
      "Loading coco annotations: 100%|███████████████| 634/634 [01:01<00:00, 10.31it/s]\n",
      " 50%|████████████████████▎                    | 314/634 [23:02<21:57,  4.12s/it]"
     ]
    }
   ],
   "source": [
    "!python data_utils/slice_coco.py --image_dir xview_dataset/train_images_rgb/ \\\n",
    "  --dataset_json_path xview_dataset/train.json \\\n",
    "  --slice_size 300 \\\n",
    "  --overlap_ratio 0.2 \\\n",
    "  --ignore_negative_samples True \\\n",
    "  --min_area_ratio 0.1 \\\n",
    "  --output_dir xview_dataset/train_images_rgb_no_neg/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503d230-cb19-4d00-b266-6ae9246b1fee",
   "metadata": {},
   "source": [
    "# 4. How to upload to s3 bucket to support distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787fde8-ce8b-4a35-b5cc-4cea5f80948a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
