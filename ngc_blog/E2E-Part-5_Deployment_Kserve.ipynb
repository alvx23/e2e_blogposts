{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a1af89-a647-404b-a400-b76f4a47ce37",
   "metadata": {},
   "source": [
    "# E2E-Part-5: Deploying Trained model on Kserve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25dbaf-5e60-4953-b3e1-7789870837ff",
   "metadata": {},
   "source": [
    " ----\n",
    "This notebook walks you each step to deploy a custom object detection model on KServe. \n",
    "\n",
    "We will show you how to:\n",
    "\n",
    "* Install Kserve Natively using Kind and Knative\n",
    "* Create a Persistent Volume Claim for local model deployment\n",
    "* Preparing custom model for Kserve inference\n",
    "* Deploying model using a KServe InferenceService\n",
    "* Complete a sample request and plot predictions\n",
    "\n",
    "Note: This notebook was tested on a Linux-based machine with Nvidia T4 GPUs. We also assume Docker is installed in your Linux system/environment\n",
    "\n",
    "Let's get started!\n",
    "\n",
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cc6b9-ae4c-44d1-9b86-b814c4d3776d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pre-reqs: Setting up Python and Jupyter Lab environment:\n",
    "Run the below commands to set up a python virtual environment, and install all the python packages needed for this tutorial\n",
    "* `sudo apt-get update && sudo apt-get  install python3.8-venv`\n",
    "* `python3 -m venv kserve_env`\n",
    "* `source kserve_env/bin/activate`\n",
    "* `pip install kserve jupyterlab torch-model-archiver torch==1.11.0 torchvision==0.12.0`\n",
    "* `jupyter lab --ip=0.0.0.0 --port=8008 --NotebookApp.token='' --NotebookApp.password=''`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177568c2-8021-4ddb-b07b-9761c12482f6",
   "metadata": {},
   "source": [
    "# Install Kserve Natively using Kind and Knative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b3ffe-8ec1-4f0c-b9ec-0ab023ec9aef",
   "metadata": {},
   "source": [
    "## Install Kind\n",
    "Open a Terminal and run the following bash commands to install a kubernetes cluster using Kind:\n",
    "* `curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64`\n",
    "* `chmod +x ./kind`\n",
    "* `sudo mv ./kind /usr/local/bin/kind`\n",
    "\n",
    "After running these commands, create a cluster by running the command: `kind create cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783960b-c498-4d8b-a9bf-4c8b79ca4b6b",
   "metadata": {},
   "source": [
    "## Install Kubectl\n",
    "Run the following bash commmands in a terminal to to install the kubectl runtime:\n",
    "* `curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"`\n",
    "* `curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"`\n",
    "* `sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bfdff-b4da-4b52-a6a7-5fbf446db900",
   "metadata": {},
   "source": [
    "## Install Kserve\n",
    "\n",
    "Run this bash script to install KServe onto our default Kubernetes Cluster, note this will install the following artifacts:\n",
    "* ISTIO_VERSION=1.15.2, KNATIVE_VERSION=knative-v1.9.0, KSERVE_VERSION=v0.9.0-rc0, CERT_MANAGER_VERSION=v1.3.0\n",
    "\n",
    "* `bash e2e_blogposts/ngc_blog/kserve_utils/bash_scripts/kserve_install.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5149ab5-60a2-4035-b61e-a08a14cdb614",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Patch Domain for local connection to KServe cluster/environment\n",
    "Run this command to patch your cluster when you want to connect to your cluster on the same machine:\n",
    "\n",
    "`kubectl patch cm config-domain --patch '{\"data\":{\"example.com\":\"\"}}' -n knative-serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e8a11-8aeb-4c28-95f3-5755d80ddd3d",
   "metadata": {},
   "source": [
    "## Run Port Forwarding to access KServe cluster\n",
    "* `INGRESS_GATEWAY_SERVICE=$(kubectl get svc --namespace istio-system --selector=\"app=istio-ingressgateway\" --output jsonpath='{.items[0].metadata.name}')`\n",
    "* `kubectl port-forward --namespace istio-system svc/${INGRESS_GATEWAY_SERVICE} 8080:80`\n",
    "\n",
    "Make sure to open a new terminal to continue the configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de97967-8e34-4c9f-8545-cb83a6ba797b",
   "metadata": {},
   "source": [
    "# Create a Persistent Volume Claim for local model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3b7db-1b18-41f4-a17c-423b6e1c25da",
   "metadata": {},
   "source": [
    "We will be creating a Persistent Volume Claim to host and access our Pytorch based Object Detection model locally. A persistent volume claim requires three k8s artifacts:\n",
    "* A Persistent Volume\n",
    "* A Persistent Volume Claim\n",
    "* A K8S pod that connects the PVC to be accessed by other K8S resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9bf2a-3e14-4654-9cb5-17d0efd606d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a Persistent Volume and Persistent Volume Claim\n",
    "Below is the yaml definition that defines the Persistent Volume (PV) and a Persistent Volume Claim (PVC). We already created a file that defines this PV in `k8s_files/pv-and-pvc.yaml`\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: task-pv-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 2Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/home/ubuntu/mnt/data\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: task-pv-claim\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "```\n",
    "\n",
    "To create the PV and PVC, run the command: `kubectl apply -f k8s_files/pv-and-pvc.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61808d13-cf54-4190-bc07-f38e8d247374",
   "metadata": {},
   "source": [
    "## Create K8s Pod to access PVC\n",
    "Below is the yaml definition that defines the K8s Pod that mounts the Persistent Volume Claim (PVC). We already created a file that defines this PV in `k8s_files/model-store-pod.yaml`\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: model-store-pod\n",
    "spec:\n",
    "  volumes:\n",
    "    - name: model-store\n",
    "      persistentVolumeClaim:\n",
    "        claimName: task-pv-claim\n",
    "  containers:\n",
    "    - name: model-store\n",
    "      image: ubuntu\n",
    "      command: [ \"sleep\" ]\n",
    "      args: [ \"infinity\" ]\n",
    "      volumeMounts:\n",
    "        - mountPath: \"/pv\"\n",
    "          name: model-store\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: \"1Gi\"\n",
    "          cpu: \"1\"\n",
    "```\n",
    "\n",
    "To create the Pod, run the command: `kubectl apply -f k8s_files/model-store-pod.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850322c-00aa-4e7f-8e18-d33cb1a7fe20",
   "metadata": {},
   "source": [
    "# Preparing custom model for Kserve inference\n",
    "\n",
    "Here we will complete some preparation steps to deploy a trained custom FasterRCNN Object Detection model using KServe. A pre-requisite is to download a checkpoint from a determined experiement. You can read this [tutorial](https://docs.determined.ai/latest/training/model-management/checkpoints.html#downloading-checkpoints-using-the-cli) on how to download a checkpoint using the Determined CLI. For this tutorial, you can download an already prepared checkpoint using the following bash command:\n",
    "\n",
    "* `wget -O kserve_utils/torchserve_utils/trained_model.pth https://determined-ai-xview-coco-dataset.s3.us-west-2.amazonaws.com/trained_model.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de4791-a00e-4129-a19f-8294b56e8edc",
   "metadata": {},
   "source": [
    "## Stripping the Checkpoint of the Optimizer State Dict\n",
    "\n",
    "Checkpoints created from a Determined Experiment will save both the model parameters and the optimizer parameters. We will need to strip the checkpoint of all parameters except the model parameters for inference. Run the bash command to generate `train_model_stripped.pth`:\n",
    "\n",
    "Run the below command in a terminal:\n",
    "\n",
    "```bash\n",
    "python kserve_utils/torchserve_utils/strip_checkpoint.py --ckpt-path kserve_utils/torchserve_utils/trained_model.pth \\\n",
    "  --new-ckpt-name kserve_utils/torchserve_utils/trained_model_stripped.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb3ec2-868c-4dcb-bb63-299671c87982",
   "metadata": {},
   "source": [
    "## Run TorchServe Export to create .mar file\n",
    "\n",
    "Run the below command to export the Pytorch Checkpoint into a .mar file that is required for torchserve inference. Our Kserve InferenceService will automatically deploy a Pod with a docker image that support TorchServe inferencing.\n",
    "\n",
    "```bash\n",
    "torch-model-archiver --model-name xview-fasterrcnn \\\n",
    "  --version 1.0 \\\n",
    "  --model-file kserve_utils/torchserve_utils/model-xview.py \\\n",
    "  --serialized-file kserve_utils/torchserve_utils/trained_model_stripped.pth \\\n",
    "  --handler kserve_utils/torchserve_utils/fasterrcnn_handler.py \\\n",
    "  --extra-files kserve_utils/torchserve_utils/index_to_name.json\n",
    "```\n",
    "\n",
    "After command finishes, run command to move file to our prepared `model-store/` directory:\n",
    "* `cp xview-fasterrcnn.mar kserve_utils/torchserve_utils/model-store -v`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a343b2-8e52-46a8-9cbc-8cd0a805a37a",
   "metadata": {},
   "source": [
    "## Copy `config/` and `model-store/` folders to the K8S PVC Pod\n",
    "\n",
    "This is the directory structure needed to prepare our custom Pytorch Model for KServe inferencing:\n",
    "```\n",
    "├── config\n",
    "│   └── config.properties\n",
    "├── model-store\n",
    "│   ├── properties.json\n",
    "│   └── xview-fasterrcnn.mar\n",
    "```\n",
    "\n",
    "Note that we have a `config/` folder that includes a config.properties. This defines A. We also have a `model-store/` directory that contains are exported models and a `properties.json` file. We need this file for B\n",
    "\n",
    "Now we will run several kubectl commands to copy over these folders into our Pod and into the PVC defined directory\n",
    "* x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6f17a-c861-4322-b5bc-bf8944115c79",
   "metadata": {},
   "source": [
    "# Deploying model using a KServe InferenceService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2712d-09b8-4aea-bfa7-138fa72ced8e",
   "metadata": {},
   "source": [
    "## Create Inference Service\n",
    "\n",
    "Below is the yaml definition that defines the KServe InferenceService that deploys models stored in the PVC. We already created a file that defines this PV in `k8s_files/torch-kserve-pvc.yaml`\n",
    "\n",
    "```yaml\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"torchserve\"\n",
    "spec:\n",
    "  predictor:\n",
    "    pytorch:\n",
    "      storageUri: pvc://task-pv-claim/\n",
    "```\n",
    "\n",
    "To create the Pod, run the command: `kubectl apply -f k8s_files/torch-kserve-pvc.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ed05a-013e-4717-bb44-f92930498133",
   "metadata": {},
   "source": [
    "## Check K8S InferenceService Logs to make sure model deployed successfully\n",
    "\n",
    "* kubectl get pods\n",
    "* kubectl logs -f <INFERENCESERVICE_POD_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706df20-65d6-4eb0-86ab-bd5f326e8296",
   "metadata": {},
   "source": [
    "# Complete a sample request and plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058da879-4716-462d-9d00-38bb64ebd6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
